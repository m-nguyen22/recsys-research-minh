{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This notebook was originally created and stored on Google Colaboratory. For convenience, output has been saved according to the intended input (stored on Google Drive). This is for display and not intended to be ran locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0--Z9eQ03Hf"
   },
   "source": [
    "# K-Means and WNMF Test Driver\n",
    "This notebook contains methods for testing various parameters of our recommender system pipeline.\n",
    "Our current hypothetical pipeline is:\n",
    "1. Review data will be clustered on the user side\n",
    "2. The clusters will be set to the server\n",
    "3. The server will append the clusters to the main matrix.\n",
    "4. The server will perform WNMF on the clusters\n",
    "5. The U and V matrices will be sent out to the users\n",
    "6. The users will have the U and V matrices multiplied to receive recommendations\n",
    "\n",
    "The full test here will mainly focus on ensuring the recommendations when updating with new clusters still provide high quality predictions (low in error). The test pipeline will be:\n",
    "1.   Take user reviews chronologically, drop the earliest reviews up to a point, and use a subset of 1000 as our utility matrix\n",
    "2.   Cluster this utility matrix with K-Means clustering on the users\n",
    "3. Perform WNMf on the clustered matrix and get the U and V matrices\n",
    "4. For the given U and V matrices, find users not within the clusters but have businesses in their review set that overlap so they can be tested on\n",
    "5. Run the tests on the users\n",
    "6. Perform steps 1 and 2 again but another set of reviews immediately after the previous set.\n",
    "7. Append the new clusters to the old clusters\n",
    "8. Perform steps 3 to 5 again.\n",
    "9. Repeat process until at least half the reviews are used. \n",
    "\n",
    "Parameters we're going to test are:\n",
    "1.   Latent factors when performing WNMF\n",
    "2.   Number of clusters per K-Means round\n",
    "3. Number of nearest neighbors when generating a recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yoIzW-BLC2D4",
    "outputId": "a215f040-3ded-4718-cb66-d9cca091a48c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mounting drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUapQDKv5vYz"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA91LAfjiCUV"
   },
   "outputs": [],
   "source": [
    "# Import review data\n",
    "fp = '/content/gdrive/My Drive/Recommender System Research Project 2019-2020/Individual Work Folder/Ian/yelp_detailed_small.csv'\n",
    "df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeHgOVzZijz4"
   },
   "outputs": [],
   "source": [
    "# Prepare initial utility matrix without earliest ratings\n",
    "\n",
    "# x is the chosen start year, where everything before is cut off\n",
    "x = 2007                              \n",
    "\n",
    "df = df.sort_values(by = 'date')\n",
    "# For each year, exclude rows whose 'date' column contains the string of the year\n",
    "for i in range(2004, x, 1):\n",
    "  df = df[~df.date.str.contains(str(i))]\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3dwmDj-6Tns"
   },
   "source": [
    "## Server Side Processes\n",
    "The \"server\" side processes include:\n",
    "1. Storing and organizing clusters into a single UV matrix\n",
    "2. Performing WNMF on the clusters\n",
    "3. Storing the U and V matrix for the \"users\"\n",
    "\n",
    "Other processes not included here but will be in the real program are: communicating U and V matrices to users. Receiving clusters from users. Mapping business ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QovA1jaGjzuS"
   },
   "source": [
    "### How to use cluster generator\n",
    "\n",
    "Create the GroupItemMatrix object which keeps track of which reviews have been clustered:\n",
    "\n",
    "`g = GroupItemMatrix(df)`\n",
    "\n",
    "Create clusters from next 1000 reviews:\n",
    "\n",
    "`g.cluster_batch()`\n",
    "\n",
    "Create k batches of clusters\n",
    "\n",
    "`g.cluster_batches(k)`\n",
    "\n",
    "Dataframe that contains all cluster batches created so far\n",
    "\n",
    "`g.df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35oQOMz5G4c2"
   },
   "outputs": [],
   "source": [
    "# Cluster generator from reading data\n",
    "\n",
    "class GroupItemMatrix:\n",
    "  def __init__(self, source_df, batch_size=1000, clusters_per_batch=25):\n",
    "    self.batch_size = batch_size\n",
    "    self.clusters_per_batch = clusters_per_batch\n",
    "\n",
    "    #keeps track of number of batches so far\n",
    "    self.batch_count = 0\n",
    "\n",
    "    #group-item matrix of cluster centroids we're creating\n",
    "    self.df = pd.DataFrame()\n",
    "\n",
    "    #source dataframe\n",
    "    self.source_df = source_df\n",
    "\n",
    "    #dataframe of used reviews: useful in cases where we need to know what reviews were used for the clusters\n",
    "    self.old_df = pd.DataFrame(columns = source_df.columns)\n",
    "\n",
    "  def cluster_batch(self):\n",
    "    #select first batch of rows (1000)  ofsource df\n",
    "    #curr_clusters is this batch of clusters\n",
    "    self.curr_clusters = self.source_df.iloc[0:self.batch_size]\n",
    "    self.source_df = self.source_df.iloc[self.batch_size:]\n",
    "\n",
    "    #append the reviews being used to old df\n",
    "    self.old_df = self.old_df.append(self.curr_clusters)\n",
    "\n",
    "    #pivot data\n",
    "    self.curr_clusters = self.curr_clusters.pivot(index = 'user_name_id', columns = 'business_name_id', values = 'stars')\n",
    "    self.curr_clusters = self.curr_clusters.fillna(value = 0)\n",
    "\n",
    "    #create KMeans clusters\n",
    "    self.m = KMeans(n_clusters = self.clusters_per_batch)\n",
    "    self.m.fit_predict(self.curr_clusters)\n",
    "\n",
    "    #record column names to use later\n",
    "    self.curr_columns = self.curr_clusters.columns\n",
    "\n",
    "    #obtain clusters\n",
    "    self.curr_clusters = pd.DataFrame(self.m.cluster_centers_)\n",
    "\n",
    "    #put the column names back in\n",
    "    self.curr_clusters.columns = self.curr_columns\n",
    "\n",
    "    #make sure index is set to consecutively increase\n",
    "    self.curr_clusters.index = [(i + (self.batch_count * self.clusters_per_batch)) for i in range(len(self.curr_clusters))]\n",
    "    self.batch_count += 1\n",
    "\n",
    "    #concatenate current batch of clusters with previous batches\n",
    "    self.df = pd.concat([self.df, self.curr_clusters], axis=0, sort=False)\n",
    "    self.df = self.df.fillna(value = 0)\n",
    "\n",
    "  #if we want to run multiple batches at one time\n",
    "  def cluster_batches(self, n):\n",
    "    for i in range(n):\n",
    "      self.cluster_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYDh4qfzHvJk"
   },
   "outputs": [],
   "source": [
    "# Server Side processes go here\n",
    "\n",
    "#takes group-item matrix df and latent factors lf\n",
    "def wnmf(df, lf):\n",
    "  #choose limit of convergence\n",
    "  limit = 0.2\n",
    "\n",
    "  iterations = min(len(df), len(df.columns)) / lf\n",
    "  #####################################################\n",
    "\n",
    "  # Save columns\n",
    "  columns = df.columns\n",
    "\n",
    "  um = df.to_numpy()\n",
    "\n",
    "  #convert NaN values to 0\n",
    "  a = np.nan_to_num(um)\n",
    "\n",
    "  #create matrix w of weights: 1 for observed values, else 0\n",
    "  w = np.zeros((len(a), len(a[0])), dtype=int)\n",
    "  for i in range(len(a)):\n",
    "      for j in range(len(a[i])):\n",
    "          if a[i][j] != 0:\n",
    "              w[i][j] = 1\n",
    "\n",
    "  #u/v matrix initialization with random values\n",
    "  u = np.random.rand(len(a), lf)\n",
    "  v = np.random.rand(lf, len(a[0]))\n",
    "\n",
    "  iteration = 0\n",
    "  prev_norm = 0\n",
    "  curr_norm = 0\n",
    "  change = 999999\n",
    "  while(iteration < iterations and change > limit):\n",
    "    u_it = np.nditer(u, flags=['multi_index'])\n",
    "    v_it = np.nditer(v, flags=['multi_index'])\n",
    "    while (not u_it.finished or not v_it.finished):\n",
    "      if not u_it.finished:\n",
    "        i, j = u_it.multi_index\n",
    "        num = np.matmul((w[i,:] * a[i,:]), v.T[:,j])\n",
    "        denom = np.matmul(w[i,:] * np.matmul(u[i,:], v), v.T[:,j])\n",
    "        u[i][j] = u_it[0] * (num / denom)\n",
    "        u_it.iternext()\n",
    "      if not v_it.finished:\n",
    "        i, j = v_it.multi_index\n",
    "        num = np.matmul(u.T[i,:], (w[:,j] * a[:,j]))\n",
    "        denom = np.matmul(u.T[i,:], (w[:,j] * np.matmul(u, v[:,j])))\n",
    "        v[i][j] = v_it[0] * (num / denom)\n",
    "        v_it.iternext()\n",
    "\n",
    "    prev_norm = curr_norm\n",
    "    curr_norm = np.linalg.norm((np.multiply(w, (a - np.matmul(u, v)))), ord='fro')\n",
    "    change = abs(curr_norm - prev_norm)\n",
    "    print('Iteration: ' + str(iteration) + ' Previous Norm: ' + str(prev_norm) + ' Current Norm: ' + str(curr_norm) + ' Change: ' + str(change))\n",
    "    iteration += 1\n",
    "\n",
    "  # Convert U and V back into a Dataframe with the proper information.\n",
    "  u = pd.DataFrame(u)\n",
    "  v = pd.DataFrame(v)\n",
    "  v.columns = columns\n",
    "\n",
    "  return u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qr7trBArG3gZ"
   },
   "source": [
    "## User Side Processes\n",
    "\n",
    "The \"user\" side processes include:\n",
    "\n",
    "1. For a particular user, take a subset of V where it only contains businesses the user rated (V2)\n",
    "2. Starting with the 3rd business in chronological order, multiply each business before it with U and V, and find out the top K similar clusters. (If it is the 3rd business, use only 1 to 2. Next run we use the 4th, so use 1 to 3 and so on until we use the last business in the overlap set)\n",
    "3. For that business, find the weighted recommendation from the top-K clusters.\n",
    "\n",
    "For testing, we will\n",
    "1. Record the error between the real rating and the predicted rating.\n",
    "2. Do this for all businesses a user has and for all users that aren't in the cluster (but have businesses in the cluster)\n",
    "\n",
    "After one cluster is done, we will expand the cluster to another 1000 ratings, and run the same tests again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujBSmaWsWeXS"
   },
   "outputs": [],
   "source": [
    "def find_candidate_reviews(g):\n",
    "  df = g.source_df            # We take the dataframe of all reviews not used in the clusters\n",
    "  exclude_users = g.old_df['user_name_id'].unique()   # We get the set of users are being used in the cluster, which we must exclude\n",
    "\n",
    "  for i in exclude_users:\n",
    "    df = df[df['user_name_id'] != i]\n",
    "\n",
    "  # We find out how many years the canidate users have made reviews in\n",
    "  user_dict = {}\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    user_id = df.iloc[i, 0]\n",
    "    rating_year = df.iloc[i, 3]\n",
    "    rating_year = rating_year[0:4]\n",
    "    \n",
    "    if user_id not in user_dict:\n",
    "      user_dict[user_id] = {rating_year}\n",
    "    else:\n",
    "      user_dict[user_id].add(rating_year)\n",
    "\n",
    "  years_per_user = []\n",
    "  for user, years in user_dict.items():\n",
    "    years_per_user.append([user, len(years)])\n",
    "\n",
    "  user_years = pd.DataFrame(years_per_user, columns=['user_name_id', 'years_with_rating'])\n",
    "\n",
    "  # We sort for the top canidates\n",
    "  user_years = user_years.sort_values(by='years_with_rating', ascending=False)\n",
    "  user_years = user_years.reset_index()\n",
    "\n",
    "  # We take the top 100 reviewers by year\n",
    "  user_years = user_years['user_name_id'].iloc[0:100]\n",
    "\n",
    "  # We find which reviews belong to the top 100 canidates so far and return that\n",
    "  df = df[df['user_name_id'].isin(user_years)]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqloofCsxToz"
   },
   "outputs": [],
   "source": [
    "def build_user_test_set(candidate_reviews, g):\n",
    "  # We reduce the reviews of canidate users down to the businesses that are in the clustered set \n",
    "  candidate_reviews = candidate_reviews[candidate_reviews['business_name_id'].isin(g.df.columns)]\n",
    "\n",
    "  dict_ub = {}\n",
    "  for i in candidate_reviews['user_name_id'].unique():\n",
    "    df_temp = candidate_reviews[candidate_reviews['user_name_id'] == i]\n",
    "\n",
    "    # We exclude anyone with less than 3 business reviews\n",
    "    if(len(df_temp) >= 3):\n",
    "      s = set(df_temp['business_name_id'])\n",
    "      dict_ub[i] = s\n",
    "  return dict_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y45csfk-3wb3"
   },
   "outputs": [],
   "source": [
    "def prediction_test(u, v, candidate_reviews, dict_ub, k):\n",
    "  error = []\n",
    "  main_v = v\n",
    "  u = u.to_numpy()\n",
    "\n",
    "  for i, j in dict_ub.items():\n",
    "    # v is the subset of (latent factors x Businesses) where the businesses \n",
    "    # are both in the volunteer set and i's (the current test user's) review set.\n",
    "    # Everytime we want to multiply by only the users businesses, we need to remake the v matrix\n",
    "    v = main_v[[x for x in j]]\n",
    "    v_columns = v.columns\n",
    "    v = v.to_numpy()\n",
    "    \n",
    "    # matmul and convert back to DataFrame  \n",
    "    uv = np.matmul(u, v)\n",
    "    uv = pd.DataFrame(uv, columns = v_columns)\n",
    "\n",
    "    # we reduce the review set down to i's reviews and sort by date\n",
    "    r = candidate_reviews[candidate_reviews['user_name_id'] == i]\n",
    "    r = r.sort_values('date')\n",
    "    r = r[r.business_name_id.isin(j)]\n",
    "\n",
    "    # We run tests from 2 to n - 1 user reviews (a user requires a minimum of 3 reviews). Each test will: run cosine similarity, find the top 3 clusters, and compare the user's review for x business.\n",
    "    \n",
    "    for x in range(2, len(r)):\n",
    "      # Select the business to test\n",
    "      currb = r.iloc[x, :]['business_name_id']\n",
    "\n",
    "      # Generate the list of businesses to run cosine similarity\n",
    "      testset = r.iloc[0:x, :]['business_name_id'].tolist()\n",
    "      l1 = [r[r['business_name_id'] == business].iloc[0]['stars'] for business in testset]\n",
    "\n",
    "      # For each cluster, we take the businesses currently being tested and run cosine similarity\n",
    "      csd = []\n",
    "      for cluster in range(len(uv)):\n",
    "        l2 = [uv.iloc[cluster][business] for business in testset]\n",
    "        csdistance = scipy.spatial.distance.cosine(l1, l2)\n",
    "        csd.append((csdistance, cluster))\n",
    "      csd.sort(reverse = True)\n",
    "      csd = csd[0:k]\n",
    "      # With the top-k clusters in csd, we generate a prediction on currb, multiplying the score from the cluster by its weight\n",
    "      weights = [cluster[0] for cluster in csd]\n",
    "      sw = sum(weights)\n",
    "      weights = map(lambda weight: weight / sw, weights)\n",
    "      prediction = 0\n",
    "      for weight, cluster in zip(weights, csd):\n",
    "        prediction += (uv.iloc[cluster[1]][currb] * weight)\n",
    "      error.append(r.iloc[x, :]['stars'] - prediction)\n",
    "\n",
    "  return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "colab_type": "code",
    "id": "0zJ5NrrpJnwx",
    "outputId": "23597cb3-ba2f-4dca-cef0-b46748670118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 latent factors, 25 clusters, and 3 k neighbors\n",
      "Iteration: 0 Previous Norm: 0 Current Norm: 19.055333270297528 Change: 19.055333270297528\n",
      "Iteration: 1 Previous Norm: 19.055333270297528 Current Norm: 14.354365371072017 Change: 4.70096789922551\n",
      "Iteration: 2 Previous Norm: 14.354365371072017 Current Norm: 12.967051392873694 Change: 1.387313978198323\n",
      "1.8143352071357106\n",
      "Iteration: 0 Previous Norm: 0 Current Norm: 30.195652348723193 Change: 30.195652348723193\n",
      "Iteration: 1 Previous Norm: 30.195652348723193 Current Norm: 24.038950905969084 Change: 6.156701442754109\n",
      "Iteration: 2 Previous Norm: 24.038950905969084 Current Norm: 22.028996131690846 Change: 2.0099547742782384\n",
      "Iteration: 3 Previous Norm: 22.028996131690846 Current Norm: 20.6311294208319 Change: 1.3978667108589455\n",
      "Iteration: 4 Previous Norm: 20.6311294208319 Current Norm: 19.484398860149618 Change: 1.1467305606822826\n",
      "3.4730510155271386\n",
      "Iteration: 0 Previous Norm: 0 Current Norm: 36.44210596040676 Change: 36.44210596040676\n",
      "Iteration: 1 Previous Norm: 36.44210596040676 Current Norm: 30.456124161222075 Change: 5.985981799184682\n",
      "Iteration: 2 Previous Norm: 30.456124161222075 Current Norm: 28.560993873877727 Change: 1.8951302873443474\n",
      "Iteration: 3 Previous Norm: 28.560993873877727 Current Norm: 27.25197940056738 Change: 1.3090144733103486\n",
      "Iteration: 4 Previous Norm: 27.25197940056738 Current Norm: 26.158624676135172 Change: 1.0933547244322064\n",
      "Iteration: 5 Previous Norm: 26.158624676135172 Current Norm: 25.19632477951333 Change: 0.9622998966218432\n",
      "Iteration: 6 Previous Norm: 25.19632477951333 Current Norm: 24.339288617490034 Change: 0.8570361620232951\n",
      "Iteration: 7 Previous Norm: 24.339288617490034 Current Norm: 23.57108737134879 Change: 0.7682012461412455\n",
      "3.2226656935450495\n",
      "Iteration: 0 Previous Norm: 0 Current Norm: 41.20615097122043 Change: 41.20615097122043\n",
      "Iteration: 1 Previous Norm: 41.20615097122043 Current Norm: 34.24915039873471 Change: 6.957000572485718\n",
      "Iteration: 2 Previous Norm: 34.24915039873471 Current Norm: 32.072467787857285 Change: 2.176682610877428\n",
      "Iteration: 3 Previous Norm: 32.072467787857285 Current Norm: 30.612015923474395 Change: 1.4604518643828897\n",
      "Iteration: 4 Previous Norm: 30.612015923474395 Current Norm: 29.42957416020754 Change: 1.182441763266855\n",
      "Iteration: 5 Previous Norm: 29.42957416020754 Current Norm: 28.41633501453405 Change: 1.0132391456734915\n",
      "Iteration: 6 Previous Norm: 28.41633501453405 Current Norm: 27.521639620552545 Change: 0.8946953939815039\n",
      "Iteration: 7 Previous Norm: 27.521639620552545 Current Norm: 26.72165747145237 Change: 0.799982149100174\n",
      "Iteration: 8 Previous Norm: 26.72165747145237 Current Norm: 25.99985975122712 Change: 0.7217977202252506\n",
      "Iteration: 9 Previous Norm: 25.99985975122712 Current Norm: 25.34432942412932 Change: 0.6555303270978001\n",
      "3.264954163382798\n",
      "Iteration: 0 Previous Norm: 0 Current Norm: 47.266211336066 Change: 47.266211336066\n",
      "Iteration: 1 Previous Norm: 47.266211336066 Current Norm: 39.51988644445405 Change: 7.746324891611948\n",
      "Iteration: 2 Previous Norm: 39.51988644445405 Current Norm: 37.07026433902035 Change: 2.449622105433704\n",
      "Iteration: 3 Previous Norm: 37.07026433902035 Current Norm: 35.53994517055166 Change: 1.5303191684686865\n",
      "Iteration: 4 Previous Norm: 35.53994517055166 Current Norm: 34.34176512275223 Change: 1.1981800477994327\n",
      "Iteration: 5 Previous Norm: 34.34176512275223 Current Norm: 33.3139568258008 Change: 1.0278082969514273\n",
      "Iteration: 6 Previous Norm: 33.3139568258008 Current Norm: 32.40502132159317 Change: 0.9089355042076335\n",
      "Iteration: 7 Previous Norm: 32.40502132159317 Current Norm: 31.58735702884273 Change: 0.8176642927504361\n",
      "Iteration: 8 Previous Norm: 31.58735702884273 Current Norm: 30.843749825875424 Change: 0.7436072029673078\n",
      "Iteration: 9 Previous Norm: 30.843749825875424 Current Norm: 30.16213233443088 Change: 0.6816174914445448\n",
      "Iteration: 10 Previous Norm: 30.16213233443088 Current Norm: 29.534035227664507 Change: 0.6280971067663721\n",
      "Iteration: 11 Previous Norm: 29.534035227664507 Current Norm: 28.95349254845673 Change: 0.5805426792077775\n",
      "Iteration: 12 Previous Norm: 28.95349254845673 Current Norm: 28.41640400819121 Change: 0.5370885402655183\n",
      "3.261207744369276\n"
     ]
    }
   ],
   "source": [
    "def rsme(ls):\n",
    "  ls = list(map(lambda x: x ** 2, ls))\n",
    "  err = sum(ls) / len(ls)\n",
    "  err = math.sqrt(err)\n",
    "  return err\n",
    "\n",
    "def test_parameters(lf, clusters, k):\n",
    "  print(\"Using {} latent factors, {} clusters, and {} k neighbors\".format(lf, clusters, k))\n",
    "  g = GroupItemMatrix(df, clusters_per_batch = clusters)\n",
    "  error = []\n",
    "  for i in range(1):  \n",
    "    g.cluster_batch()\n",
    "    u, v = wnmf(g.df, lf)\n",
    "    candidate_reviews = find_candidate_reviews(g)\n",
    "    test_set = build_user_test_set(candidate_reviews, g)\n",
    "    #error.append(prediction_test(u, v, candidate_reviews, test_set, k))\n",
    "    error = prediction_test(u, v, candidate_reviews, test_set, k)\n",
    "    print(rsme(error))\n",
    "  # print(rsme(error))\n",
    "\n",
    "test_parameters(10, 25, 3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "K-Means and WNMF Test Driver",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
